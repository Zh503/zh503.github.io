---
title: codeBERT
date: 2024-04-17 17:25:25
permalink: /pages/f4d2ca/
categories:
  - 学习笔记
  - 论文译文
tags:
  - 
author: 
  name: hao
  link: https://github.com/zh503
---
# CodeBERT: A Pre-Trained Model for Programming and Natural Languages

## 1.摘要

我们提出了CodeBERT，这是一个面向编程语言（PL）和自然语言（NL）的双模态预训练模型。CodeBERT学习了支持自然语言与编程语言之间各种应用的通用表示，比如自然语言代码搜索、代码文档生成等。我们采用基于Transformer的神经架构开发了CodeBERT，并训练它使用融合了替换标记检测预训练任务的混合目标函数，即检测生成器中采样的合理替代项。这使我们能够同时利用“双模态”数据和“单模态”数据，前者为模型训练提供输入标记，后者有助于学习更好的生成器。我们通过微调模型参数对CodeBERT进行了两项自然语言-编程语言应用的评估。结果表明，CodeBERT在自然语言代码搜索和代码文档生成方面实现了最先进的性能。此外，为了了解CodeBERT学习了哪种类型的知识，我们构建了一个用于自然语言-编程语言探测的数据集，并在固定预训练模型参数的零样本设置下进行评估。结果显示，CodeBERT在自然语言-编程语言探测方面优于先前的预训练模型。

## 引言

大型预训练模型，如ELMo（Peters等，2018年），GPT（Radford等，2018年），BERT（Devlin等，2018年），XLNet（Yang等，2019年）和RoBERTa（Liu等，2019年）已经显著提高了各种自然语言处理（NLP）任务的最新技术水平。这些预训练模型从大规模未标记文本中通过自监督目标进行优化学习了有效的上下文表示，例如掩码语言建模，它从人为掩码的输入序列中预测原始掩码单词。在NLP中预训练模型的成功也带来了多模态预训练模型的激增，如ViLBERT（Lu等，2019年）用于语言-图像和VideoBERT（Sun等，2019年）用于语言-视频，它们是从语言-图像对等双模态数据学习的，并采用了双模态自监督目标。

在这项工作中，我们提出了CodeBERT，一个用于自然语言（NL）和编程语言（PL）（如Python，Java，JavaScript等）的双模态预训练模型。CodeBERT捕捉了自然语言和编程语言之间的语义联系，并生成了通用表示，可以广泛支持NL-PL理解任务（如自然语言代码搜索）和生成任务（如代码文档生成）。它采用了多层Transformer（Vaswani等，2017年），该架构被大多数大型预训练模型采用。为了充分利用NL-PL对的双模态实例以及大量可用的单模态代码，我们使用混合目标函数对CodeBERT进行训练，包括标准掩码语言建模（Devlin等，2018年）和替换标记检测（Clark等，2020年），其中单模态代码有助于学习更好的生成器，以生成更好的替代标记。

我们使用Github代码仓库中的代码对CodeBERT进行训练，涵盖了6种编程语言。其中，双模态数据点是与函数级自然语言文档配对的代码（Husain等人，2019）。训练过程与多语言BERT（Pires等人，2019）类似，即为6种编程语言学习一个预训练模型，而无需使用明确的标记来表示输入编程语言。我们对CodeBERT进行了两个下游任务的评估，包括自然语言代码搜索和代码文档生成。结果表明，细调CodeBERT的参数在这两个任务上实现了最先进的性能。为了进一步探究CodeBERT学习到了哪种类型的知识，我们构建了一个用于NL-PL探测的数据集，并在零炮制场景下测试了CodeBERT，即在没有对CodeBERT的参数进行细调的情况下。我们发现，CodeBERT在性能上持续优于纯自然语言的预训练模型RoBERTa。本研究的贡献如下：
• CodeBERT是首个面向多种编程语言的大规模NL-PL预训练模型。
• 实证结果表明，CodeBERT在代码搜索和代码转文本生成任务中均有效。
• 我们进一步创建了一个数据集，这是第一个用于研究基于代码的预训练模型的探测能力的数据集。

## 2.背景

2.1 Pre-Trained Models in NLP 

近几年，大型预训练模型（Peters等人，2018年；Radford等人，2018年；Devlin等人，2018年；Yang等人，2019年；Liu等人，2019年；Raffel等人，2019年）在几乎所有自然语言处理任务上带来了显著的经验改进。成功的方法在大规模纯文本上用自监督学习目标训练深度神经网络。最具代表性的神经架构之一是Transformer（Vaswani等人，2017年），也是本文使用的框架。它包含多个自注意力层，并且可以传统地使用梯度下降以端到端的方式学习，因为每个组件都是可微分的。术语“自监督”意味着用于预训练的监督信息是从原始数据中自动收集而非手动注释的。主导的学习目标是语言建模及其变种。例如，在GPT（Radford等人，2018年）中，学习目标是语言建模，即根据前文单词{w1，w2，...，wk−1}预测下一个词wk。由于预训练的最终目标不是训练一个好的语言模型，因此考虑前后文来学习更好的通用语境表示是很有必要的。这引出了BERT（Devlin等人，2018年）中使用的屏蔽语言建模目标，该目标是在给定周围上下文的情况下学会预测一系列屏蔽单词的结果。屏蔽语言建模也被用作训练CodeBERT的两个学习目标之一。

2.2 多模态预训练模型Multi-Modal Pre-Trained Models
预训练模型在自然语言处理领域取得了显著的成功，这推动了多模态预训练模型的发展，这些模型可以学习不同模态输入之间的隐含对齐关系。这些模型通常通过双模态数据进行学习，例如语言-图像对或语言-视频对。例如，ViLBERT（Lu等，2019）通过图像标题数据进行学习，模型通过重构被遮盖的图像区域的类别或遮盖的单词来学习，并同时预测描述图像内容的标题。类似地，VideoBERT（Sun等，2019）通过语言-视频数据进行学习，并通过视频和文本的遮盖标记预测来进行训练。我们的工作属于这一领域的研究方向，我们将自然语言（NL）和程序语言（PL）视为不同的模态。我们的方法与以往的工作不同之处在于，模型训练的数据不仅包括NL-PL对的双模态数据，还包括大量的单模态数据，例如没有配对文档的代码数据。
一项同时进行的研究（Kanade等，2019）使用遮盖语言建模和下一句预测作为目标，在Python源代码上训练了一个BERT模型，其中一句话被定义为Python标准下的逻辑代码行。在预训练过程方面，CodeBERT与他们的工作不同之处在于（1）CodeBERT以跨模态的方式进行训练，利用了双模态的NL-PL数据和单模态的PL/NL数据，（2）CodeBERT在六种编程语言上进行了预训练，（3）CodeBERT使用了基于替换令牌检测的新的学习目标进行训练。

## 3 CodeBERT

在本节中，我们详细描述了CodeBERT的细节，包括模型架构、输入和输出表示、用于训练CodeBERT的目标和数据，以及在应用于下游任务时如何微调CodeBERT。

### 3.1 模型架构

我们按照BERT（Devlin等人，2018）和RoBERTa（Liu等人，2019）的方式，使用多层双向Transformer（Vaswani等人，2017）作为CodeBERT的模型架构。我们不会详细讨论普遍存在的Transformer架构。我们通过使用与RoBERTa-base完全相同的模型架构来开发CodeBERT。模型参数的总数为125M。

![image-20240411105741814](https://s2.loli.net/2024/04/11/aYXq1WGd8pZhFgI.png)

### 3.2 输入/输出表示

在预训练阶段，我们将输入设置为两个片段的连接，使用一个特殊的分隔符标记，即[CLS]，w1，w2，..wn，[SEP ]，c1，c2，...，cm，[EOS]。其中一个片段是自然语言文本，另一个片段是某种编程语言的代码。[CLS]是位于这两个片段前面的特殊标记，其最终的隐藏表示被视为用于分类或排序的汇总序列表示。按照Transformer中处理文本的标准方式，我们将自然语言文本视为单词序列，并将其分割为WordPiece（Wu等人，2016）。我们将一段代码视为标记序列。
CodeBERT的输出包括（1）每个标记的上下文向量表示，包括自然语言和代码，以及（2）[CLS]的表示，它作为汇总序列表示。

### 3.3 预训练数据

我们使用自然语言-代码对的平行数据和独立的代码及文字配对的双模数据来训练 CodeBERT。具体说来，我们使用来自 Github 仓库的数据点，其中每个双模数据点是一个具有配对文档的单一函数，每个独立代码是一个没有配对文档的函数。具体来说，我们使用了 Husain 等人（2019）提供的最新大型数据集，跨六种编程语言（Python、Java、JavaScript、PHP、Ruby 和 Go），包括 210 万个双模数据点和 640 万个独立代码。数据统计见表 1。

![image-20240411105758061](https://s2.loli.net/2024/04/11/oeaAmjSM6uRCYIi.png)

> 图1：一对NL-PL的示例，其中NL是功能文档（用黑色虚线标示）中的第一段文字（用红色填充）。

这些数据来自公开的、非分支的 GitHub 仓库，并经过一系列约束和规则的筛选。例如，（1）每个项目应被至少一个其他项目使用，（2）文档被截断为第一段，（3）小于三个令牌的文档被删除，（4）小于三行的函数被删除，（5)包含子字符串“test”的函数名被删除。数据的示例见图 1。

### 3.4 CodeBERT的预训练 

我们在这里描述用于训练CodeBERT的两个目标。第一个目标是遮蔽语言建模（MLM），在文献中已被证明是有效的。我们将遮蔽语言建模应用于自然语言-编程语言对的双模态数据上。第二个目标是替换标记检测（RTD），它进一步使用大量的非对称态数据，例如没有配对自然语言文本的代码。有关模型预训练的详细超参数请参见附录B.1。

![image-20240411105915772](https://s2.loli.net/2024/04/11/dQ9CYmIji3UFDuc.png)

> 图 2：关于替换令牌检测目标的示意图。自然语言（NL）和代码生成器都是语言模型，根据周围的上下文生成掩蔽位置的可信令牌。NL-Code 判别器是目标预训练模型，通过检测从 NL 和 PL 生成器中采样的可信替代令牌来进行训练。NL-Code 判别器用于在微调阶段生成通用表示。在微调阶段，NL 和代码生成器都被弃用。